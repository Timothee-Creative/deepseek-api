## ğŸš€ **Ollama(DEEPSEEK) API - AI Chatbot Server**
A simple **Express.js API** that connects to **Ollama's locally hosted LLMs** (Large Language Models), allowing you to chat with AI models like **deepseek-r1:7b**.

---

### **ğŸ“Œ Features**
âœ… **Local AI API** â€“ No need for an internet connection, runs directly on your machine.  
âœ… **Custom AI Models** â€“ Uses **deepseek-r1:7b** by default (configurable via `.env`).  
âœ… **Fast API Requests** â€“ Responses are generated quickly using **Ollama's API**.  
âœ… **Secure & Lightweight** â€“ Uses **CORS** and **environment variables** for flexibility.  

---

## ğŸ“¦ **Installation & Setup**
### **1ï¸âƒ£ Install Ollama**
If you haven't installed Ollama yet, follow the official [installation guide](https://ollama.ai/download).


### **2ï¸âƒ£Clone the Repository**
git clone https://github.com/akinmiday/deepseek-api.git
cd ollama-api


### **3ï¸âƒ£ Install Dependencies**
```sh
npm install
```

### **4ï¸âƒ£ Create a `.env` File**
```sh
cp .env.example .env
```
Edit the `.env` file and set your preferred values:
```
PORT=5000
OLLAMA_API_URL=http://localhost:11434/api/generate
OLLAMA_BASE_URL=http://localhost:11434/api
OLLAMA_DEFAULT_MODEL=deepseek-r1:7b
```

### **5ï¸âƒ£ Start Ollama in the Background**
```sh
ollama serve
```

### **6ï¸âƒ£ Run the API Server**
```sh
node server.js
```

---

## ğŸ”¥ **API Endpoints**
Below are the available **API endpoints**.

### **1ï¸âƒ£ Check API Status**
#### **â¡ï¸ GET `/`**
Returns a confirmation message that the API is running.

#### **âœ… Example Response**
```json
{
  "message": "âœ… Ollama API is running with model: deepseek-r1:7b"
}
```

---

### **2ï¸âƒ£ Get Available AI Models**
#### **â¡ï¸ GET `/models`**
Retrieves a list of all available **Ollama models** installed on your machine.

#### **âœ… Example Response**
```json
{
  "models": [
    { "name": "deepseek-r1:7b", "size": "7B" },
    { "name": "mistral:7b", "size": "7B" }
  ]
}
```

---

### **3ï¸âƒ£ Generate AI Response**
#### **â¡ï¸ POST `/chat`**
Generates a response using the default AI model (**deepseek-r1:7b** or whatever is set in `.env`).

#### **ğŸ“Œ Request Body**
```json
{
  "prompt": "Explain quantum physics in simple terms.",
  "stream": false
}
```

#### **âœ… Example Response**
```json
{
  "response": "Quantum physics is the study of very small particles..."
}
```

ğŸ”¹ **Note:**  
- `prompt` â€“ Required field, AI's input.  
- `stream` â€“ Optional, set to `true` for **streaming responses**.

---

### **4ï¸âƒ£ Pull a Model from Ollama**
#### **â¡ï¸ POST `/pull-model`**
Downloads the **default model** (set in `.env`) from Ollama Hub.

#### **âœ… Example Response**
```json
{
  "success": true,
  "message": "âœ… Model deepseek-r1:7b pulled successfully"
}
```

---

## ğŸ¨ **Custom Startup Banner**
When you start the API (`node index.js`), you'll see a **custom ASCII banner** like this:
```
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— 
  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
  â•šâ•â•     â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•  â•šâ•â•

  ğŸš€ Ollama API is running! Access it at: http://localhost:5000  
  ğŸ¤– Model Loaded: deepseek-r1:7b  
  ğŸ–¥ï¸ Running on: my-laptop | darwin | arm64  
  ğŸ”„ Restart to refresh server logs.
```

## **Screenshots**
![Image Description](./screenshot/image1.png)
![Image Description](./screenshot/image2.png)


---

## ğŸ›  **Troubleshooting**
### **âŒ 1. Ollama API Not Found**
```sh
Error: connect ECONNREFUSED 127.0.0.1:11434
```
**Solution:** Make sure Ollama is running. Start it using:
```sh
ollama serve
```

### **âŒ 2. Failed to Fetch Models**
If `/models` returns an error:
```json
{
  "error": "Failed to fetch available models"
}
```
**Solution:** Ensure models are installed:
```sh
ollama list
```
If the model is missing, install it:
```sh
ollama pull deepseek-r1:7b
```

### **âŒ 3. API Server Doesn't Start**
```sh
Error: Port 5000 already in use
```
**Solution:** Change the `PORT` in `.env`, or kill the process:
```sh
lsof -i :5000
kill -9 <PID>
```

---

## ğŸš€ **Contributing**
Want to improve this project? Feel free to fork, submit a PR, or suggest features! ğŸ˜Š

---

## ğŸ“œ **License**
This project is open-source under the **MIT License**.

---

## ğŸŒŸ **Like this? Give it a Star!** â­  
If you find this project useful, **leave a star on GitHub!** ğŸš€âœ¨

---

### **ğŸ’¬ Questions?**
- **Twitter/X:** [@akinmiday](https://x.com/akinmiday)  
- **GitHub Issues:** [Create an Issue](https://github.com/akinmiday/deepseek-api/issues)  

---

### ğŸ”¥ **Now Youâ€™re Ready to Chat with AI Locally!**  
**Enjoy your AI chatbot powered by Ollama! ğŸš€ğŸ”¥**
